{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "id": "hlRar1zxmFnS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2vF0H6UQ6Tf",
        "outputId": "8a25bc4b-1c89-4ba7-84a5-5e8fa5a9248c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 173,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_dWhJ5Zo3xX"
      },
      "source": [
        "# Encoder Decoder Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "id": "NUCm6yVQ0GAC"
      },
      "outputs": [],
      "source": [
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 2000 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "eval_iters = 200\n",
        "n_embd = 36\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "vocab_size = 1503"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyICTDyUysqF"
      },
      "source": [
        "### Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "id": "WVIfInBtnCtQ"
      },
      "outputs": [],
      "source": [
        "class InputEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, dim_embd=n_embd):\n",
        "        super().__init__()\n",
        "        self.dim_embd = dim_embd\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding = nn.Embedding(vocab_size, dim_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embedding(x) * (self.dim_embd ** 0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "id": "U6_RjR42qD_s"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, num_toks ,dim_embd= n_embd):\n",
        "        super().__init__()\n",
        "        self.dim_embd = dim_embd\n",
        "        self.toks_len = num_toks\n",
        "        self.pos_embedding = nn.Embedding(num_toks, dim_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        print(\"in pos enc: \", x.shape)\n",
        "        return x + self.pos_embedding(torch.arange(x.shape[1], device=x.device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRgFJUNdyvKE"
      },
      "source": [
        "### Encoder and Decoder Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "id": "RTpkucAtWeqZ"
      },
      "outputs": [],
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embd_size, dim_embd = n_embd):\n",
        "        super().__init__()\n",
        "\n",
        "        self.key = nn.Linear(dim_embd, embd_size, bias=False)\n",
        "        self.query = nn.Linear(dim_embd, embd_size, bias=False)\n",
        "        self.value = nn.Linear(dim_embd, embd_size, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x , enc_out=None):\n",
        "\n",
        "        B,T,E = x.shape\n",
        "\n",
        "        if enc_out is None:\n",
        "          k = self.key(x)   # (B, T, embd_size)\n",
        "          q = self.query(x) # (B, T, embd_size)\n",
        "          v = self.value(x) # (B, T, embd_size)\n",
        "        else:\n",
        "          k = self.key(enc_out)\n",
        "          q = self.query(x)\n",
        "          v = self.value(enc_out)\n",
        "\n",
        "        scaled_dot_prod = (q @ k.transpose(-2, -1)) * (k.shape[-1] ** -0.5) # (B, T, embd_size) @ (B, embd_size, T) -> (B, T, T)\n",
        "        soft = F.softmax(scaled_dot_prod , dim=-1) # (B, T, T)\n",
        "        attention_score = self.dropout(soft)\n",
        "\n",
        "        output = attention_score @ v # (B, T, T) @ (B, T, embd_size) -> (B, T, embd_size)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "id": "ATvb3W9CWeqZ"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "      def __init__(self, num_heads, each_embd_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([SelfAttention(each_embd_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(each_embd_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "      def forward(self, x, en_out=None):\n",
        "        out = torch.cat([h(x, en_out) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "id": "JGcmiTXHub2Q"
      },
      "outputs": [],
      "source": [
        "class MaskedSelfAttention(nn.Module):\n",
        "    def __init__(self, embd_size, dim_embd= n_embd):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(dim_embd, embd_size, bias=False)\n",
        "        self.query = nn.Linear(dim_embd, embd_size, bias=False)\n",
        "        self.value = nn.Linear(dim_embd, embd_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        B,T,E = x.shape\n",
        "        k = self.key(x)   # (B, T, embd_size)\n",
        "        q = self.query(x) # (B, T, embd_size)\n",
        "        v = self.value(x) # (B, T, embd_size)\n",
        "\n",
        "        scaled_dot_prod = (q @ k.transpose(-2, -1)) * (k.shape[-1] ** -0.5) # (B, T, embd_size) @ (B, embd_size, T) -> (B, T, T)\n",
        "        mask = scaled_dot_prod.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        soft = F.softmax(mask, dim=-1) # (B, T, T)\n",
        "        attention_score = self.dropout(soft)\n",
        "\n",
        "        output = attention_score @ v # (B, T, T) @ (B, T, embd_size) -> (B, T, embd_size)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "id": "mEzI5L2L3AHG"
      },
      "outputs": [],
      "source": [
        "class MaskedMultiHeadAttention(nn.Module):\n",
        "      def __init__(self, num_heads, each_embd_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([MaskedSelfAttention(each_embd_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(each_embd_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "      def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsG9MuMU4Xls"
      },
      "source": [
        "### Feed Forward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "id": "5EbERfHj4SlL"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, dim_embd=n_embd, ff_hid_dim=2048):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(dim_embd, ff_hid_dim)\n",
        "        self.linear2 = nn.Linear(ff_hid_dim, dim_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.linear1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPsOWzUs5wKL"
      },
      "source": [
        "### Block of Encoder-Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "id": "1Ztd5UimXa0T"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, num_heads, dim_embd=n_embd):\n",
        "        super().__init__()\n",
        "        head_size = dim_embd // num_heads\n",
        "        self.mha = MultiHeadAttention(num_heads, head_size)\n",
        "        self.ff = FeedForward(dim_embd)\n",
        "        self.ln1 = nn.LayerNorm(dim_embd)\n",
        "        self.ln2 = nn.LayerNorm(dim_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.mha(self.ln1(x))\n",
        "        x = x + self.ff(self.ln2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "id": "VnCDIveL5s_R"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, num_heads, dim_embd=n_embd):\n",
        "        super().__init__()\n",
        "        head_size = dim_embd // num_heads\n",
        "        self.mmha = MaskedMultiHeadAttention(num_heads, head_size)\n",
        "        self.mha = MultiHeadAttention(num_heads, head_size)\n",
        "        self.ff = FeedForward(dim_embd)\n",
        "        self.ln1 = nn.LayerNorm(dim_embd)\n",
        "        self.ln2 = nn.LayerNorm(dim_embd)\n",
        "        self.ln3 = nn.LayerNorm(dim_embd)\n",
        "\n",
        "    def forward(self, x, en_out=None):\n",
        "        x = x + self.mmha(self.ln1(x))\n",
        "        x = x + self.mha(self.ln2(x), en_out=en_out)\n",
        "        x = x + self.ff(self.ln3(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HbjLKKe790z"
      },
      "source": [
        "# Encoder Decoder Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "id": "-EykjS-zZFK-"
      },
      "outputs": [],
      "source": [
        "class EncoderModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # self.embedding = InputEmbedding(vocab_size)\n",
        "        # self.positional_encoding = PositionalEncoding(block_size)\n",
        "\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.positional_encoding = nn.Embedding(block_size, n_embd)\n",
        "\n",
        "        self.blocks = nn.Sequential(*[EncoderBlock(n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.linear_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, input, targets=None):\n",
        "        B, T = input.shape\n",
        "\n",
        "        # print(\"Encoder Before Embedding\")\n",
        "        # tok_emb = self.embedding(input)  # (B,T,embd_size)\n",
        "        # print(\"Encoder After Embedding\")\n",
        "        # pos_emb = self.positional_encoding(tok_emb) # (B,T,embd_size)\n",
        "        # print(\"Encoder After Positional Embd\")\n",
        "\n",
        "        # print(\"Encoder Before Embedding: \", input.shape)\n",
        "        tok_emb = self.token_embedding_table(input)\n",
        "        # print(\"Encoder After Embedding: \", tok_emb.shape)\n",
        "        # print(\"T size: \", T)\n",
        "        pos_emb = self.positional_encoding(torch.arange(T, device=device))\n",
        "        # print(\"Encoder After Positional Embd: \", pos_emb.shape)\n",
        "        x = tok_emb + pos_emb\n",
        "        # print(\"x in Encoder After Positional Embd\", x.shape)\n",
        "\n",
        "        # print(\"Enc after embd: \", x.shape)\n",
        "\n",
        "        x = self.blocks(x) # (B,T,embd_size)\n",
        "        x = self.ln_f(x) # (B,T,embd_size)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "id": "66Zdd2EZ72bt"
      },
      "outputs": [],
      "source": [
        "class DecoderModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # self.embedding = InputEmbedding(vocab_size)\n",
        "        # self.positional_encoding = PositionalEncoding(block_size)\n",
        "\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.positional_encoding = nn.Embedding(block_size, n_embd)\n",
        "\n",
        "        self.blocks = nn.ModuleList([DecoderBlock(n_head) for _ in range(n_layer)])\n",
        "\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.linear_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, input, en_out, targets=None):\n",
        "\n",
        "        B, T = input.shape\n",
        "\n",
        "        # # print(\"Decoder Before Embedding\")\n",
        "        # tok_emb = self.embedding(input)  # (B,T,embd_size)\n",
        "        # # print(\"Decoder Token emb:\", tok_emb.shape)\n",
        "        # pos_emb = self.positional_encoding(tok_emb) # (B,T,embd_size)\n",
        "        # # print(\"Decoder Pos emb:\", pos_emb.shape)\n",
        "\n",
        "        tok_emb = self.token_embedding_table(input)\n",
        "        pos_emb = self.positional_encoding(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block(x, en_out)\n",
        "            # print(\"DecoderEach block shape: \", x.shape)\n",
        "\n",
        "        x = self.ln_f(x) # (B,T,embd_size)\n",
        "        # print(\"Layer norm shape: \", x.shape)\n",
        "        logits = self.linear_head(x) # (B,T,vocab)\n",
        "        # print(\"Linear head shape: \", logits.shape)\n",
        "\n",
        "        # For inference\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVum_ejWl1d_"
      },
      "source": [
        "# Translation Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "id": "RcTUFdRbg94q"
      },
      "outputs": [],
      "source": [
        "class TranslationModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.encoder = EncoderModel()\n",
        "        self.decoder = DecoderModel()\n",
        "\n",
        "    def forward(self, enc_input, dec_input, targets=None):\n",
        "        # print(\"Before Encoder\")\n",
        "        en_out = self.encoder(enc_input)\n",
        "        # print(\"After Encoder shape: \", en_out.shape)\n",
        "        logits, loss = self.decoder(dec_input, en_out, targets=targets)\n",
        "        # print(\"After Decoder shape: \", logits.shape)\n",
        "        return logits, loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "id": "BOcg5_B4Pm25"
      },
      "outputs": [],
      "source": [
        "model = TranslationModel().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2wMupRf_XEN"
      },
      "source": [
        "# Training Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "id": "MZuFarlsJOkK"
      },
      "outputs": [],
      "source": [
        "!pip install -q datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "id": "twxsa4BoJCzH"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "data = load_dataset(\"CohleM/english-to-nepali\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMH73468JZ0q",
        "outputId": "32c757e7-7e57-4e98-8bbe-3bc956258042"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('Technical committees will be attached to each ministry.',\n",
              " 'प्रत्येक मन्त्रालय अन्तर्गत शिल्प (टेक्निकल) कमिटीहरु गठन गरिनेछन्')"
            ]
          },
          "execution_count": 190,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data['train']['en'][0], data['train']['ne'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "id": "6TzNjqyCKm3b"
      },
      "outputs": [],
      "source": [
        "eng_data = data['train']['en']\n",
        "nep_data = data['train']['ne']\n",
        "\n",
        "eng_corpus = \" \".join(eng_data)\n",
        "nep_corpus = \" \".join(nep_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "id": "oGgwolVyLpDS"
      },
      "outputs": [],
      "source": [
        "class BPETokenizer:\n",
        "\n",
        "    def __init__(self, text, vocab_size = 300):\n",
        "        tokens = text.encode(\"utf-8\")\n",
        "        tokens = list(map(int, tokens))\n",
        "        self.merges = self.create_merges(tokens,vocab_size)\n",
        "        self.vocab = {idx: bytes([idx]) for idx in range(256)}\n",
        "        for (p0, p1), idx in self.merges.items():\n",
        "            self.vocab[idx] = self.vocab[p0] + self.vocab[p1]\n",
        "\n",
        "    def get_stats(self, ids):\n",
        "        counts = {}\n",
        "        for pair in zip(ids, ids[1:]):\n",
        "            counts[pair] = counts.get(pair, 0) + 1\n",
        "        return counts\n",
        "\n",
        "    def merge(self, ids, pair, idx):\n",
        "        newids = []\n",
        "        i = 0\n",
        "        while i < len(ids):\n",
        "          if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
        "            newids.append(idx)\n",
        "            i += 2\n",
        "          else:\n",
        "            newids.append(ids[i])\n",
        "            i += 1\n",
        "        return newids\n",
        "\n",
        "    def create_merges(self, ids, vocab_size):\n",
        "        num_merges = vocab_size - 256\n",
        "        merges = {}\n",
        "        for i in range(num_merges):\n",
        "          stats = self.get_stats(ids)\n",
        "          pair = max(stats, key=stats.get)\n",
        "          idx = 256 + i\n",
        "          print(f\"merging {pair} into a new token {idx}\")\n",
        "          ids = self.merge(ids, pair, idx)\n",
        "          merges[pair] = idx\n",
        "        return merges\n",
        "\n",
        "\n",
        "    def encode(self, text):\n",
        "        tokens = list(text.encode(\"utf-8\"))\n",
        "        while len(tokens) >= 2:\n",
        "          stats = self.get_stats(tokens)\n",
        "          pair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))\n",
        "          if pair not in self.merges:\n",
        "            break # nothing else can be merged\n",
        "          idx = self.merges[pair]\n",
        "          tokens = self.merge(tokens, pair, idx)\n",
        "        return tokens\n",
        "\n",
        "    def decode(self, ids):\n",
        "        # given ids (list of integers), return Python string\n",
        "        tokens = b\"\".join(self.vocab[idx] for idx in ids)\n",
        "        text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
        "        return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "id": "iCdA09tVMFdi"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Reading pickle file\n",
        "\n",
        "with open(\"eng_tokenizer_50k.pkl\", \"rb\") as file:\n",
        "    eng_tok = pickle.load(file)\n",
        "\n",
        "with open(\"nep_tokenizer_50k.pkl\", \"rb\") as file:\n",
        "    nep_tok = pickle.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "id": "TwNTIQS_jdPo"
      },
      "outputs": [],
      "source": [
        "vocab = eng_tok.vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "collapsed": true,
        "id": "Y47osOilj6Eo"
      },
      "outputs": [],
      "source": [
        "# string_dict = {key: value.decode('utf-8', errors='replace') for key, value in vocab.items()}\n",
        "\n",
        "# # Print the resulting dictionary\n",
        "# for key, val in string_dict.items():\n",
        "#   print(key, val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {
        "id": "MienPW5BemcR"
      },
      "outputs": [],
      "source": [
        "# Adding eos and sos in vocab\n",
        "\n",
        "nep_tok.vocab[1501] = b'<sos>'\n",
        "nep_tok.vocab[1502] = b'<eos>'\n",
        "nep_tok.vocab[1500] = b'<pad>'\n",
        "eng_tok.vocab[1500] = b'<pad>'\n",
        "eng_tok.vocab[1501] = b'<sos>'\n",
        "eng_tok.vocab[1502] = b'<eos>'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeqPvN5jXc65",
        "outputId": "249e8309-ddbf-4c0a-8755-e3b66ea9a419"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1503, 1503)"
            ]
          },
          "execution_count": 197,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(eng_tok.vocab), len(nep_tok.vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAytrMR5ZU6R",
        "outputId": "f6e6e08a-83c2-47c2-8dda-87cfbc9cd610"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnbxdYJcYPOx",
        "outputId": "cf60915f-ee29-42f6-c51b-a26d439589ac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-199-fd73761a62c3>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  enco_eng_data = torch.load(\"/content/drive/MyDrive/Models/enco_eng_data.pth\")\n",
            "<ipython-input-199-fd73761a62c3>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  deco_nep_data = torch.load(\"/content/drive/MyDrive/Models/deco_nep_data.pth\")\n"
          ]
        }
      ],
      "source": [
        "enco_eng_data = torch.load(\"/content/drive/MyDrive/Models/enco_eng_data.pth\")\n",
        "deco_nep_data = torch.load(\"/content/drive/MyDrive/Models/deco_nep_data.pth\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {
        "id": "U6DhjaB9UAiG"
      },
      "outputs": [],
      "source": [
        "eng_data_train = enco_eng_data\n",
        "deco_nep_data_sos = [[1501] + sentence for sentence in deco_nep_data]\n",
        "deco_nep_data_eos = [sentence + [1502] for sentence in deco_nep_data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {
        "id": "SFYeeFaA0DMC"
      },
      "outputs": [],
      "source": [
        "# Function to pad a batch of sequences\n",
        "def pad_batch(batch, pad_token=1500):\n",
        "    \"\"\"\n",
        "    Pads input, target, and output sequences dynamically based on max length in the batch.\n",
        "    \"\"\"\n",
        "    input_seqs, target_seqs, output_seqs = zip(*batch)  # Unpack batch\n",
        "\n",
        "    # Find max length for this batch\n",
        "    max_len = max(max(len(seq) for seq in input_seqs),\n",
        "                  max(len(seq) for seq in target_seqs),\n",
        "                  max(len(seq) for seq in output_seqs))\n",
        "\n",
        "    # Apply padding\n",
        "    padded_input = [seq + [pad_token] * (max_len - len(seq)) for seq in input_seqs]\n",
        "    padded_target = [seq + [pad_token] * (max_len - len(seq)) for seq in target_seqs]\n",
        "    padded_output = [seq + [pad_token] * (max_len - len(seq)) for seq in output_seqs]\n",
        "\n",
        "    # Convert to tensors\n",
        "    return torch.tensor(padded_input), torch.tensor(padded_target), torch.tensor(padded_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {
        "id": "VUwrUiJ17TX0"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Dataset class remains the same\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, input_data, target_data, output_data):\n",
        "        self.input_data = input_data\n",
        "        self.target_data = target_data\n",
        "        self.output_data = output_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_data[idx], self.target_data[idx], self.output_data[idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {
        "id": "CbyvN7309wl-"
      },
      "outputs": [],
      "source": [
        "# Create Dataset\n",
        "dataset = TranslationDataset(enco_eng_data, deco_nep_data_sos, deco_nep_data_eos)\n",
        "\n",
        "# Create DataLoader with custom collate_fn\n",
        "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=pad_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPE38OIR_RuQ",
        "outputId": "ee357b10-6e0a-43da-a905-423d20f8a6a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 1\n",
            "Input tensor shape: torch.Size([16, 129])\n",
            "Target tensor shape: torch.Size([16, 129])\n",
            "Output tensor shape: torch.Size([16, 129])\n",
            "Batch 2\n",
            "Input tensor shape: torch.Size([16, 125])\n",
            "Target tensor shape: torch.Size([16, 125])\n",
            "Output tensor shape: torch.Size([16, 125])\n",
            "Batch 3\n",
            "Input tensor shape: torch.Size([16, 125])\n",
            "Target tensor shape: torch.Size([16, 125])\n",
            "Output tensor shape: torch.Size([16, 125])\n"
          ]
        }
      ],
      "source": [
        "for batch_idx, (input_tensor, target_tensor, output_tensor) in enumerate(train_loader):\n",
        "    print(f\"Batch {batch_idx + 1}\")\n",
        "    print(\"Input tensor shape:\", input_tensor.shape)\n",
        "    print(\"Target tensor shape:\", target_tensor.shape)\n",
        "    print(\"Output tensor shape:\", output_tensor.shape)\n",
        "    if batch_idx == 2:  # Check only the first few batches\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNRoqtEwTo-p"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "KR1AI-3nHavH",
        "outputId": "7bcd32c9-661f-4daf-e46f-59e9e8ec6826"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/5: 100%|██████████| 3125/3125 [08:00<00:00,  6.50it/s, loss=2.7133]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch [1/5] - Avg Loss: 2.6848\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/5:  31%|███       | 963/3125 [02:32<06:07,  5.88it/s, loss=1.6487]"
          ]
        }
      ],
      "source": [
        "train_loss = []\n",
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "\n",
        "    # Initialize tqdm progress bar for the epoch\n",
        "    progress_bar = tqdm(train_loader, total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    for batch_idx, (input_tensor, target_tensor, output_tensor) in enumerate(progress_bar):\n",
        "        input_tensor, target_tensor, output_tensor = (\n",
        "            input_tensor.to(device),\n",
        "            target_tensor.to(device),\n",
        "            output_tensor.to(device)\n",
        "        )\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits, loss = model(input_tensor, target_tensor, targets=output_tensor)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Update tqdm bar with loss information\n",
        "        progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
        "\n",
        "    torch.save(model.state_dict(), \"/content/drive/MyDrive/Models/translation_model.pth\")\n",
        "\n",
        "\n",
        "    # Compute and store average loss for the epoch\n",
        "    avg_epoch_loss = total_loss / len(train_loader)\n",
        "    train_loss.append(avg_epoch_loss)\n",
        "\n",
        "    # Print final loss after epoch completes\n",
        "    print(f\"\\nEpoch [{epoch+1}/{num_epochs}] - Avg Loss: {avg_epoch_loss:.4f}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZ1RMN7GM2j2"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(train_loss)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "p2wMupRf_XEN",
        "_L-eJSfBT4EN"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
